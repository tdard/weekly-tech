---
title: "Episode 1: r√©soudre une t√¢che d'Aspect Based Sentiment Analysis - Pr√©liminaires"
date: 2023-03-06T17:07:53+01:00
draft: false
tags: ["data science", "sentiment analysis", "absa"]
summary: "Dans cette s√©rie, nous allons tenter de r√©soudre un probl√®me d'analyse de sentiments, en nous basant sur des 
jeux de donn√©es de r√©f√©rence et des techniques de l'√©tat de l'art."
---
# üìÇ Episodes pr√©c√©dents

_Sur la m√™me th√©matique_:
- {{< page_link path="/posts/2023-S6" >}}
- {{< page_link path="/posts/2023-S7" >}}

S√©rie _r√©soudre une t√¢che d'Aspect Based Sentiment Analysis_:
- **{{< page_link path="/posts/2023-S8" >}}**

# üìù Mise en contexte
L√†, on en a marre. Th√©orie par-ci, explications fumeuses par l√†; quand est-ce qu'on pratique?

Et bien Marcel mets ta ceinture, parce qu'on va se lancer dans une grande aventure: celle de r√©soudre une t√¢che 
d'_Aspect Based Sentiment Analysis_ (ABSA).

Laquelle? Et bien comme on a √©t√© curieux, on sait d√©sormais qu'il y a plusieurs challenges phares dans ce domaine: 
c'est les √©v√®nements _SemEval-20XX_ (pour _Semantic Evaluation_), d√©clin√©s en plusieurs mill√©simes de 2014 √† 2017. 

Ceux qui se croient malin feront la remarque qu'il en existe d'autres jusque 2023 (et oui, on en fait une cuv√©e par an), 
mais je leur rappelerai que les t√¢ches de ces challenges ne sont pas en lien avec l'ABSA.

Comme on aime la modernit√©, on va s'int√©resser seulement au petit dernier, le **SemEval-2017**. 




# ‚öôÔ∏è Mise en place de l'environnement

**C'est quoi ce  challenge? Qu'est-ce qu'on va y faire?**

Alors c'est tr√®s simple. Le challenge SemEval, c'est un √©v√®nement qui propose une liste de sujets centr√©s sur la 
th√©matique d'√©valuation s√©mantique, aka. dire des choses sur du texte. Il y a plusieurs sujets, et dans chaque sujets on 
retrouve des sous-probl√®mes. Chaque sujet est grosso-modo un probl√®me g√©n√©ral (par exemple _Sentiment Analysis in 
Twitter_), et chaque sous-probl√®me aborde une facette du sujet. 


Alors direction le [site internet](https://alt.qcri.org/semeval2017/index.php?id=tasks) du challenge, regardons les 
probl√®mes √† r√©soudre associ√©s √†:


{{< figure src="figure_1.png"  width="400">}}

Dans notre cas particulier, on a l'embarras du choix:
- Message Polarity Classification: Given a message, classify whether the message is of positive, negative, or neutral 
sentiment.
- Topic-Based Message Polarity Classification [...]
- Tweet quantification: Given a set of tweets about a given topic, estimate the distribution of the tweets [...]

Comme nous nous int√©ressons avant-tout aux probl√®mes d'_ABSA_, prenons le parti de choisir le second challenge, d√©crit 
ci-dessous:

![figure_2](figure_2.png)

Lan√ßons nous sur cette t√¢che, t√©l√©chargeons les donn√©es et c'est parti!

# üõ¢ Les donn√©es


On remarque cinq fichiers dans l'archive de donn√©es

{{<figure src="figure_3.png" height="150">}} 
En y jetant un oeil et en lisant le README on se rend compte que:
- **baseline-B-english.txt** est un fichier regroupant les pr√©dictions de la m√©thode baseline, on y 
voit 3 colonnes: identifiant de tweet, topic et polarit√©  
- **SemEval2017-task4-dev.subtask-BD.english.INPUT.txt** est le dataset _train_, on y retrouve les m√™mes colonnes ainsi 
qu'une de plus, contenant le texte des tweets
- **twitter-2016test-BD-English.txt** se fait appeler le "GOLD file", et qui est en fait un copier-coller du dataset de 
train, sans la derni√®re colonne. On a juste les v√©rit√©s terrains quoi ‚ö†Ô∏èEn revanche, il s'agit des r√©sultats de... 2016, 
donc on s'en fiche
- **SemEval2017_task4_test_scorer_subtaskB.pl** est un script `perl` permettant de calculer les m√©triques du challenge 
√† partir de pr√©dictions. Celles-ci sont: _macro-average Recall_, _macro-average F1_, et _Accuracy_

**Remarques**
- D'apr√®s le [compte rendu](https://aclanthology.org/S16-1002.pdf) du challenge de l'ann√©e pr√©c√©dente, la m√©thode baseline 
est le classifier qui attribue √† chaque tweet une polarit√© positive.
- Le macro-averaged est d√©fini de fa√ßon particuli√®re, puisqu'il ne s'agit pas d'un recall classique. Suit la formulation 
du recall:
  - \\( R_{topic} = \frac{1}{2}( R_{topic, N} + R_{topic, P}) \\) avec \\( R_{topic, X} \\) le rappel pour le topic et 
la polarit√© X
  - \\( R = \frac{1}{N} \sum_{topic} R_{topic} \\)




_Un exemple du dataset d'entra√Ænement:_
{{<figure src="figure_5.png">}}

# üìä Evaluation

Premi√®re chose √† faire, v√©rifions que le script d'√©valuation fonctionne:

```shell
perl SemEval2017_task4_test_scorer_subtaskB.pl \
  SemEval2017-task4-dev.subtask-BD.english.INPUT.txt \  # PREDICTION
  SemEval2017-task4-dev.subtask-BD.english.INPUT.txt  # VERITE TERRAIN
```

On obtient les r√©sultats suivants:

![figure_4](figure_4.png)

Et un fichier .scored est cr√©√© avec les m√©triques en question:
````text
	positive: P=1.0000, R=1.0000, F1=1.0000
	negative: P=1.0000, R=1.0000, F1=1.0000
		AvgF1_2=1.000, AvgR_3=1.000, Acc=1.000
	OVERALL SCORE : 1.000
````

Tout est logg√©, les m√©triques sont au max, donc top on peut continuer.

Que donne la baseline en revanche? Effectuons le m√™me test par la commande suivante:

````shell
perl SemEval2017_task4_test_scorer_subtaskB.pl \
  baseline-B-english.txt \                            
  SemEval2017-task4-dev.subtask-BD.english.INPUT.txt

````

On obtient ces r√©sultats:
````text
	positive: P=1.0000, R=0.7783, F1=0.8753
	negative: P=0.0000, R=0.0000, F1=0.0000
		AvgF1_2=0.438, AvgR_3=0.389, Acc=0.778
	OVERALL SCORE : 0.438
````

Hum, notre baseline fonctionne bien pour pr√©dire les polarit√©s positives (il a un bon F1), mais il se gauffre 
compl√®tement en pr√©disant des classes n√©gatives.


# üóì Et ensuite

Quoi, c'est d√©j√† fini?

Eh oui, on avance doucement mais s√ªrement, ce serait dommage de se cramer les ailes en si bon chemin non?

Au prochain √©pisode, on ira charger les donn√©es avec python üêç, jouer un peu avec et ~~avec de la chance~~, on obtiendra 
des premiers r√©sultats; j'ai h√¢te de vous les montrer!  

Allez, √† la semaine prochaine!

---

