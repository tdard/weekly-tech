---
title: "Episode 3: r√©soudre une t√¢che d'Aspect Based Sentiment Analysis - Baseline et 1√®re mod√©lisation"
date: 2023-03-29T09:44:40+02:00
draft: false
tags: ["data science", "sentiment analysis", "absa"]
resources:
- name: spec_1
  src: spec_1.vg.json
featured_image: "meme accuracy.jpeg"
summary: "Dans cet √©pisode, nous allons construire une baseline pour nous assurer d'√™tre bien partis, nous 
introduirons √©galement un mod√®le un peu moins b√™te nous permettant de nous comparer aux participants du challenge"
---

## üìÇ Episodes pr√©c√©dents

_Sur la m√™me th√©matique_:
- {{< page_link path="/posts/2023-S6" >}}
- {{< page_link path="/posts/2023-S7" >}}

S√©rie _r√©soudre une t√¢che d'Aspect Based Sentiment Analysis_:
- {{< page_link path="/posts/2023-S8" >}}
- {{< page_link path="/posts/2023-S9" >}}
- **{{< page_link path="/posts/2023-S10" >}}**

## üìå Sommaire

- üî´ Introduction
- üëÄ Oh la boulette
- üîßChoix de mod√©lisation
- üöÄ Let's code
- üìñ En r√©sum√©

## üî´ Introduction

Alors, me revoila assez vite pour vous pr√©senter la mod√©lisation qui suit l'analyse exploratoire effectu√©e pr√©c√©demment.

Au risque de d√©plaire √† certains, nous allons commencer sobrement avec des m√©thodes tr√®s classiques.

Pourquoi donc? Pour la bonne raison qu'on ne se d√©barasse pas d'un moustique avec un h√©licopt√®re d'attaque; on commence 
par tenter de le zigouiller avec ses mains, avant de se laisser tenter par la tapette √† mouches, la raquette 
√©lectrique, le napalm ou le bazooka pour les plus ha√Æneux d'entre nous.

Ici c'est pareil, notre pipeline _scikit-learn_ simple et basique sera facile √† mettre en place, et on va pouvoir 
v√©rifier rapidement l√† o√π il est mauvais. Cela nous permettra de passer √† l'√©tape suivante, en tentant de palier √† ses 
limites.

## üëÄ Oh la boulette

Cher lecteur, ch√®re lectrice, j'ai p√™ch√©. En pensant explorer les bonnes donn√©es, nous (la faute est collective bien 
√©videmment) avons en fait vu le dataset de _test_ de l'ann√©e pr√©c√©dente (SemEval 2016)... 

![meme accuracy](meme%20accuracy.jpeg)

**Par chance, j'ai pu retrouver le package collector int√©gral, avec les data de _train_, _dev_, _devtest_ et 
_test_.**

Pour la petite anecdote, les donn√©es de _train_, _dev_ et _devtest_ sont les m√™mes pour les challenges de 2016 et 2017;
seul les jeu de test sont diff√©rents. 

Comme les donn√©es de test ne sont _normalement_ pas trop √©loign√©es de celles qui servent √† entrainer et valider nos 
mod√®les, on va partir du principe que notre analyse faite pr√©c√©demment reste valable. Ouf!

On remarque tout de m√™me deux √©l√©ments importants: 
- Les topics ne sont pas du tout les m√™mes pour chaque dataset.
- Quand un tweet n'est pas r√©cup√©r√© (parce que supprim√© ou indisponible), son texte est mis √† "Not Available"


> Les donn√©es d'entra√Ænement et validation sont les m√™mes que pour la comp√©tition SemEval2016, vous pouvez trouver
> les donn√©es facilement accessibles sur 
> [GitHub](https://github.com/balikasg/SemEval2016-Twitter_Sentiment_Evaluation/tree/master/src/subtaskD/data)

## üîßChoix de mod√©lisation

On a pu voir dans un article pr√©c√©dent les diff√©rents paradigmes de mod√©lisation, ici nous allons tenter une approche 
_SeqClass_ (classification de phrase) dont les features seront calcul√©es sur une base de _bag-of-words_.

> C'est quoi un _bag-of-words_? Eh bien Jamy, un _sac de mots_ c'est une repr√©sentation vectorielle d'un texte (ici un 
> _tweet_), o√π chaque √©l√©ment correspond √† un _token_ de ton vocabulaire et la valeur associ√©e √† cet √©l√©ment vaut 0 si 
> le _token_ n'appara√Æt pas dans le texte, et 1 sinon.

Notre mod√©lisation restera classique (pour l'instant üëÄ):
- _Bag-of-words_ boost√© par du _TF-IDF_ *
- R√©gression Logistique 

En faisant cela, nous effectuons deux hypoth√®ses fortes:
- La pr√©sence de certains mots est l'unique d√©terminant de la polarit√©
- Comme on a moins de 1% des tweets concernant plusieurs topics, on ne prendra pas en compte le topic dans les features 
de notre mod√®le

On fera attention √† quelques d√©tails n√©anmoins, pour g√©rer l'_imbalance_ du _dataset_ concernant le nombre de tweets √† 
polarit√© n√©gative par rapport aux positifs.

> *C'est quoi le TF-IDF? En bref, un mot est consid√©r√© important: (1) quand il appara√Æt dans peu de documents; (2) quand 
> il appara√Æt de nombreuses fois dans un m√™me document

## üöÄ Let's code

En bon cuisinier, pr√©parons notre recette pour une mod√©lisation r√©ussie. 
Alors pour **entra√Æner un mod√®le et analyser ses r√©sultats**, il nous faut:
1. Charger les donn√©es, les nettoyer et les convertir dans un format convenable
2. Instancier un mod√®le, puis l'entra√Æner
3. Calculer des m√©triques pour nous informer sur la qualit√© des r√©sultats
4. Sauvegarder les pr√©dictions fausses quelque part pour les analyser individuellement

Ici on s'en fiche du 4√® point parce qu'on est des voyous, et qu'on le fera plus tard dans un autre article.

### Donn√©es requises

- Le dataset train/dev/devtest: un bon samaritain l'a t√©l√©charg√© [ici](https://github.com/balikasg/SemEval2016-Twitter_Sentiment_Evaluation/tree/master/src/subtaskD/data)
- Le dataset test: un zip est disponible [ici](http://alt.qcri.org/semeval2017/task4/data/uploads/semeval2017-task4-test.zip)

### Pr√©paration des donn√©es

Comme nous ne sommes pas des cochons mais que nous d√©veloppons notre mod√®le dans un notebook (belle antith√®se 
remarquez-vous ü§°), nous mettrons en place des fonctions utilitaires pour rendre notre code plus clean.

#### Chargement

_utils.py_
````python
import pandas as pd


def load_data(path: str) -> pd.DataFrame: 
    data = pd.read_csv(path, sep="\t", header=None)
    if len(data.columns) == 5:
        data = data.drop(columns=[4])
    data = data.rename(columns={0: "tweet_id", 1: "topic", 2: "polarity", 3: "tweet_text"})
    return data
````

Par malchance, le dataset de test (t√©l√©chargeable dans la section **RESULTS** [ici](https://alt.qcri.org/semeval2017/task4/index.php?id=data-and-tools)) 
n'est pas correctement charg√© en utilisant pandas (allez savoir pourquoi, seulement 1069 lignes sur 1185 sont trouv√©es).

On code donc un autre utilitaire pour charger ces donn√©es:


````python
def load_test(path: str) -> pd.DataFrame:
    with open(path) as file:
        records = [l.strip().split("\t") for l in file.readlines()]
    test = pd.DataFrame.from_records(records, columns=["tweet_id", "topic", "polarity", "tweet_text"])
    return test
````

_notebook.ipynb_
```python
%loadext autoreload
%autoreload 2
```

````python
from utils import *

TRAIN_DATA_PATH = "data/subtaskBD.downloaded.tsv"  # Les fichiers ont √©t√© renomm√©s et d√©plac√©s dans un folder commun
DEV_DATA_PATH = "data/subtaskBD.dev.downloaded.all.tsv"
DEVTEST_DATA_PATH = "data/100_topics_XXX_tweets.topic-two-point.subtask-BD.devtest.gold.downloaded.txt"
TEST_DATA_PATH = "data/SemEval2017-task4-test.subtask-BD.english.txt"
````

````python
train_data = load_data(TRAIN_DATA_PATH)
dev_data = load_data(DEV_DATA_PATH)
devtest_data = load_data(DEVTEST_DATA_PATH)
test_data = load_test(TEST_DATA_PATH)

assert len(train_data.columns) == len(dev_data.columns) == len(devtest_data.columns) == len(test_data.columns)

train_data.head()
````

{{< figure src="chart_2.png" height="150">}}

#### Data cleaning & explicitation des features et targets

On veut enlever les tweets dont le texte est mis √† "Not Available", et convertir notre dataframe pandas en une 
collection de tableaux numpy que l'on pourra traiter par notre (futur) mod√®le:


_utils.py_
````python
def prepare_data(
    data: pd.DataFrame, 
    polarity_to_int: dict[str, int]
) -> tuple[np.ndarray]:
    """
    Clean data, and extracts arrays that will be used.
    
    :returns: a tuple of arrays (ids, topics, polarity, X, y)
    """
    df = data.loc[data.tweet_text != "Not Available"]
    return tuple([
        *(df[c].values for c in df.columns),
        df["polarity"].apply(lambda p: polarity_to_int[p]).values
    ])
````

Dans notre environnement jupyter, d√©finissons deux param√®tres importants:
- Le mapping entre la polarit√© et la cible binaire (0 ou 1)
- La _seed_ permettant de fixer l'al√©atoire dans notre exp√©rimentation

_notebook.ipynb_
```python
POLARITY_TO_INT = {
    "positive": 1, 
    "negative": 0
}
SEED = 42
```

Appelons ensuite la fonction utilitaire dans le notebook:

_notebook.ipynb_
```python
ids_train, topics_train, polarity_train, X_train, y_train = prepare_data(train_data, POLARITY_TO_INT)
ids_val, topics_val, polarity_val, X_val, y_val = prepare_data(dev_data, POLARITY_TO_INT)
ids_devtest, topics_devtest, polarity_devtest, X_devtest, y_devtest = prepare_data(devtest_data, POLARITY_TO_INT)
ids_test, topics_test, polarity_test, X_test, y_test = prepare_data(test_data, POLARITY_TO_INT)
```

Donnons nous √©galement une id√©e de la proportion des classes dans notre dataset r√©sultant:

_utils.py_
````python
import altair as alt
import numpy as np


def get_target_statistics(labels: np.ndarray, log_scale: bool, title: str) -> alt.Chart:
    values, counts = np.unique(labels, return_counts=True)
    data = pd.DataFrame({"values":values, "counts": counts})
    
    base = alt.Chart(
        data, 
        title=title, 
        width=150
    ).encode(
        x=alt.X("values:O", title="Target value"), 
        y=alt.Y("counts:Q", title="Count", scale=alt.Scale(type="log" if log_scale else "linear")), 
        color=alt.condition(alt.datum.values == "positive", alt.value("green"), alt.value("red"))
    )
    
    bars = base.mark_bar()
    text = base.mark_text(dy=-10).encode(text="counts:Q")
    return bars + text
````

_notebook.ipynb_
````python
(get_target_statistics(polarity_train, False, "train") | \
get_target_statistics(polarity_val, False, "dev") | \
get_target_statistics(polarity_devtest, False, "devtest") | \
get_target_statistics(polarity_test, False, "test")).properties(
  title="R√©partition de la polarit√© en fonction du dataset"
)
````

Le graphe obtenu a cette allure:

{{< vega file="spec_1" div_id="spec_1" >}}

On constate que les jeux de _train_, _dev_ et _devtest_ sont relativement homog√®nes en termes de r√©partition 
positif/n√©gatif. En revanche, le jeu de test (que l'on ne devrait pas avoir sous les yeux en th√©orie üëÄ) montre une 
tendance totalement oppos√©e, puisqu'il y a laregment plus de tweets n√©gatifs que de positifs. 

Mais bon, faisons comme si nous n'avions rien vus sur ce dernier jeu de donn√©es ü´£

### D√©finition du mod√®le et entra√Ænement

#### Premi√®re √©tape: baseline
Pour s'assurer qu'on ne fait pas de b√™tises, il faut absolument cr√©er un mod√®le baseline identique √† celui explicit√© 
dans le challenge, et v√©rifier que ses performances sont identiques:


_notebook.ipynb_
```python
pipeline = make_pipeline(
    DummyClassifier(strategy="constant", constant=POLARITY_TO_INT["positive"])
)
pipeline = pipeline.fit(X_train, y_train)  # It does nothing here
```

#### Seconde √©tapes: m√©triques

En nous aidant des d√©finitions √©crites dans le [papier](https://alt.qcri.org/semeval2017/task4/data/uploads/semeval2017-task4.pdf) 
de recherche du challenge, nous allons impl√©menter (ou utiliser) les m√©triques requises:

- \\( AvgRecall = \frac{1}{2} (Recall^P + Recall^N) \\)
- \\( F_1^{PN} = \frac{1}{2} (F_1^P + F_1^N) \\)
- Accuracy

Qui sont basiquement des macro-averaged recall & F1 score, et accuracy.

_utils.py_
````python
from sklearn.metrics import recall_score, f1_score, accuracy_score


def recall_pn(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    recall_positive = recall_score(y_true, y_pred, zero_division=0)
    recall_negative = recall_score(1 - y_true, 1 - y_pred, zero_division=0)
    return 0.5 * (recall_positive + recall_negative)


def f1_pn(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    f1_positive = f1_score(y_true, y_pred, zero_division=0)
    f1_negative = f1_score(1 - y_true, 1 - y_pred, zero_division=0)
    return 0.5 * (f1_positive + f1_negative)
````

D√©sormais, nous pouvons √©valuer le notre classifier baseline sur le jeu de test:

_notebook.ipynb_
````python
test_preds = pipeline.predict(X_test)

test_accuracy = accuracy_score(y_test, test_preds)
test_macro_averaged_recall = recall_pn(y_test, test_preds)
test_macro_averaged_f1 = f1_pn(y_test, test_preds)

print(f"Accuracy: {test_accuracy:.3f}; Recall: {test_macro_averaged_recall:.3f}; F1: {test_macro_averaged_f1:.3f}")
````

En guise de r√©sultats, nous avons:
```
Recall: 0.500; F1: 0.285; Accuracy: 0.398
```
Ce qui correspond effectivement √† la baseline sp√©cifi√©e dans le tableau ci-dessous (baseline B1, avant derni√®re ligne):

{{< figure src="chart_1.png" height="400" >}}

> Pour rappel, le tableau est disponible dans le [papier scientifique du challenge](https://alt.qcri.org/semeval2017/task4/data/uploads/semeval2017-task4.pdf)

Magnifico! Passons √† la suite en cr√©ant un v√©ritable classifier cette fois-ci.

#### Troisi√®me √©tape: un premier vrai classifier
Cr√©ons notre pipeline _scikit-learn_; faisons attention √† √©quilibrer les performances entre polarit√© n√©gative et 
positive en attribuant un poids plus important sur les exemples n√©gatifs:

_notebook.ipynb_
```python
pipeline = make_pipeline(
    TfidfVectorizer(), 
    LogisticRegression(class_weight="balanced", random_state=SEED)
)
pipeline_trained = pipeline.fit(X_train, y_train)
```

Toujours dans le notebook, effectuons l'√©valuation sur chaque dataset:

_utils.py_
````python
def evaluate_estimator(estimator, X, y_true):
    y_pred = estimator.predict(X)
    
    accuracy = accuracy_score(y_true, y_pred)
    avg_recall = recall_pn(y_true, y_pred)
    avg_f1 = f1_pn(y_true, y_pred)

    print(f"Recall: {avg_recall:.3f}; F1: {avg_f1:.3f}; Accuracy: {accuracy:.3f}")
````

_notebook.ipynb_
````python
evaluate_estimator(pipeline, X_train, y_train)
evaluate_estimator(pipeline, X_val, y_val)
evaluate_estimator(pipeline, X_devtest, y_devtest)
evaluate_estimator(pipeline, X_test, y_test)
````
Les r√©sultats sont les suivants:
```
# TRAIN 
Recall: 0.944; F1: 0.898; Accuracy: 0.935
# DEV/VALIDATION
Recall: 0.680; F1: 0.679; Accuracy: 0.746
# DEVTEST
Recall: 0.709; F1: 0.692; Accuracy: 0.797
# TEST
Recall: 0.701; F1: 0.682; Accuracy: 0.683
```

Mise √† part l'overfitting violent que l'on d√©guste (-20% sur chaque m√©trique), on remarque tout de m√™me que nos performances sont tr√®s honorables, 
puisque les r√©sultats sur le jeu de test nous positionnerait en 18√® place du classement vu plus haut.


Pas mal pour un premier essai non? Alors attendez la suite, on fera encore mieux!


## üìñ En r√©sum√©
- On s'est gourr√©s de dataset les pr√©c√©dents √©pisodes, mais c'est pas grave parce qu'on reste sur les m√™mes types de donn√©es
- On s'est donn√©s pour objectif de faire tourner un premier mod√®le bien simple et de l'√©valuer, ce qu'on a fait
- On s'est assur√©s de pas faire n'importe quoi en codant et en √©valuant une des baselines du challenge
- On a eu de bons r√©sultats!

Et maintenant, on fait quoi?


Alors je sais pas toi mais je suis tent√© par tuner notre mod√®le pour voir jusqu'o√π on peut pousser les performances d'un 
algorithme bien simple, puis par la suite partir sur quelque chose de plus exotique üçâ comme des LLM type BERT ou GPT.


A la prochaine!

---