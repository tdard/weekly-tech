---
title: "Episode 3: r√©soudre une t√¢che d'Aspect Based Sentiment Analysis - Mod√©lisation"
date: 2023-03-29T09:44:40+02:00
draft: true
tags: ["data science", "sentiment analysis", "absa"]
resources:
- name: spec_1
  src: spec_1.vg.json
---

## üìÇ Episodes pr√©c√©dents

_Sur la m√™me th√©matique_:
- {{< page_link path="/posts/2023-S6" >}}
- {{< page_link path="/posts/2023-S7" >}}

S√©rie _r√©soudre une t√¢che d'Aspect Based Sentiment Analysis_:
- {{< page_link path="/posts/2023-S8" >}}
- {{< page_link path="/posts/2023-S9" >}}
- **{{< page_link path="/posts/2023-S10" >}}**


## Introduction

Alors, me revoila assez vite pour vous pr√©senter la mod√©lisation qui suit l'analyse exploratoire effectu√©e pr√©c√©demment.

Au risque de d√©plaire √† certains, nous allons commencer sobrement avec des m√©thodes tr√®s classiques.

Pourquoi donc? Pour la bonne raison qu'on ne se d√©barasse pas d'un moustique avec un h√©licopt√®re d'attaque; on commence 
par tenter de le zigouiller avec ses mains, avant de se laisser tenter par la tapette, la raquette 
√©lectrique, le napalm ou le bazooka pour les plus ha√Æneux d'entre nous.

Ici c'est pareil, notre pipeline _scikit-learn_ simple et basique sera facile √† mettre en place, et on va pouvoir 
v√©rifier rapidement l√† o√π il est mauvais. Cela nous permettra de passer √† l'√©tape suivante, en tentant de palier √† ses 
limites.



## üîßChoix de mod√©lisation

On a pu voir dans un article pr√©c√©dent les diff√©rents paradigmes de mod√©lisation, ici nous allons tenter une approche 
_SeqClass_ (classification de phrase) dont les features seront calcul√©es sur une base de _bag-of-words_.

> C'est quoi un _bag-of-words_? Eh bien Jamy, un _sac de mots_ c'est une repr√©sentation vectorielle d'un texte (ici un 
> _tweet_), o√π chaque √©l√©ment correspond √† un _token_ de ton vocabulaire et la valeur associ√©e √† cet √©l√©ment vaut 0 si 
> le _token_ n'appara√Æt pas dans le texte, et 1 sinon.

Notre mod√©lisation restera classique (pour l'instant üëÄ):
- _Bag-of-words_ suppl√©ment√© par du _TF-IDF_ 
- R√©gression Logistique 

Pour cela on effectue deux hypoth√®ses fortes:
- La pr√©sence de certains mots est l'unique facteur d√©terminant de la polarit√©
- Comme on a moins de 1% des tweets concernant plusieurs topics, on ne prendra pas en compte le topic dans les features 
de notre mod√®le

On fera attention √† quelques d√©tails n√©anmoins, pour g√©rer l'_imbalance_ du _dataset_ concernant le nombre de tweets √† 
polarit√© n√©gative par rapport aux positifs.

> C'est quoi le TF-IDF? En bref, un mot est consid√©r√© important: (1) quand il appara√Æt dans peu de documents; (2) quand 
> il appara√Æt de nombreuses fois dans un m√™me document

## üöÄ Let's code

Tel Machiavel, pr√©m√©ditons notre m√©fait en planifiant. Pour **entra√Æner un mod√®le et analyser ses r√©sultats**, il nous 
faut:
1. Charger les donn√©es dans un format convenable
2. Diviser celles-ci en un jeu d'entra√Ænement et un de validation (disons que l'on s'en fiche d'en avoir un 3√® de test 
comme on reste tr√®s simple)
3. Instancier le mod√®le, puis l'entra√Æner
4. Calculer des m√©triques pour nous informer sur la qualit√© des r√©sultats
5. Sauvegarder les pr√©dictions fausses quelque part pour les analyser individuellement



### Pr√©paration des donn√©es

Comme nous ne sommes pas des cochons mais que nous d√©veloppons notre mod√®le dans un notebook (oxymore), nous mettrons en 
place des fonctions utilitaires pour rendre notre code plus clean.

_utils.py_
````python
import pandas as pd

def load_data(path: str) -> pd.DataFrame: 
    data = pd.read_csv(path, sep="\t", header=None)
    data = data.drop(columns=[4])
    data = data.rename(columns={0: "tweet_id", 1: "topic", 2: "polarity", 3: "tweet_text"})
    return data
````

_notebook.ipynb_
```python
%loadext autoreload
%autoreload 2
```

````python
from utils import *

TRAIN_DATA_PATH = "SemEval2017-task4-dev.subtask-BD.english.INPUT.txt"
````

````python
data = load_data(TRAIN_DATA_PATH)
data.head()
````


### Train/Validation split

On veut diviser nos donn√©es en un jeu d'entra√Ænement et de validation; avec pour objectif de r√©cup√©rer:
- Les features (X)
- Les cibles/target (y)
- Les labels correspondant √† chaque cible

On fera attention √† faire en sorte que la distribution de polarit√© dans le jeu d'entra√Ænement et celle dans celui de 
validation soient √©gales: on le fait grace au param√®tre _stratify_ de la fonction _train_test_split_.

_utils.py_
````python
def prepare_data(
    data: pd.DataFrame, 
    id_column: str,  # tweet ID
    topic_column: str,
    polarity_column: str,
    text_column: str, 
    polarity_to_int: dict[str, int], 
    train_size: float, 
    seed: int
):
    id_ = data[id_column]  
    X = data[text_column]
    labels = data[polarity_column]
    y = labels.apply(lambda p: polarity_to_int.get(p))
    
    X_train, X_val, y_train, y_val, labels_train, labels_val, id_train, id_val = train_test_split(
        X, 
        y, 
        labels, 
        id_,
        train_size=train_size, 
        stratify=y,  # Stratify to deal with imbalanced classes
        random_state=seed
    )
    
    return (X_train, X_val, 
            y_train, y_val, 
            labels_train, labels_val, 
            id_train, id_val)
````

Dans notre environnement jupyter, d√©finissons trois param√®tres importants:
- La taille du jeu d'entra√Ænement (ici 70% des donn√©es disponibles)
- Le mapping entre la polarit√© et la cible binaire (0 ou 1)
- La _seed_ permettant de fixer l'al√©atoire dans notre exp√©rimentation

_notebook.ipynb_
```python
TRAIN_SIZE = 0.7
POLARITY_TO_INT = {
    "positive": 1, 
    "negative": 0
}
SEED = 42
```

Appelons ensuite la fonction utilitaire dans le notebook:

_notebook.ipynb_
```python
(
    X_train, X_val, 
    y_train, y_val, 
    labels_train, labels_val, 
    id_train, id_val
) = prepare_data(
    data=data_subset, 
    id_column="tweet_id",
    topic_column="topic",
    polarity_column="polarity",
    text_column="tweet_text",
    polarity_to_int=POLARITY_TO_INT,
    train_size=TRAIN_SIZE
)
print(f"{len(X_train)}: length of training data")
print(f"{len(X_val)}: length of validation data")
```

Donnons nous √©galement une id√©e de la proportion des classes dans notre dataset r√©sultant:

_utils.py_
````python
import altair as alt
import numpy as np


def get_target_statistics(labels: np.ndarray, log_scale: bool, title: str) -> alt.Chart:
    values, counts = np.unique(labels, return_counts=True)
    data = pd.DataFrame({"values":values, "counts": counts})
    
    base = alt.Chart(
        data, 
        title=title, 
        width=150
    ).encode(
        x=alt.X("values:O", title="Target value"), 
        y=alt.Y("counts:Q", title="Count", scale=alt.Scale(type="log" if log_scale else "linear")), 
        color=alt.condition(alt.datum.values == "positive", alt.value("green"), alt.value("red"))
    )
    
    bars = base.mark_bar()
    text = base.mark_text(dy=-10).encode(text="counts:Q")
    return bars + text
````

_notebook.ipynb_
````python
get_target_statistics(labels_train, log_scale=False, title="Train target statistics") | \
get_target_statistics(labels_val, log_scale=False, title="Validation target statistics")
````

Le graphe obtenu a cette allure:

{{< vega file="spec_1" div_id="spec_1" >}}

On constate que notre jeu d'entrainement et de validation ont chacun environ 30% d'exemples n√©gatifs, ce qui est correct 
au regard des r√©sultats obtenus dans l'analyse exploratoire.

### D√©finition du mod√®le et entra√Ænement

Cr√©ons notre pipeline _scikit-learn_; faisons attention √† √©quilibrer les performances entre polarit√© n√©gative et 
positive en attribuant un poids plus important sur les exemples n√©gatifs:

_notebook.ipynb_
```python
pipeline = make_pipeline(
    TfidfVectorizer(), 
    LogisticRegression(class_weight="balanced")
)
```

````python
pipeline_trained = pipeline.fit(X_train, y_train)
````
